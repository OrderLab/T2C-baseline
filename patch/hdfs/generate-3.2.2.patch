diff --git a/hadoop-common-project/hadoop-common/src/main/bin/hadoop-functions.sh b/hadoop-common-project/hadoop-common/src/main/bin/hadoop-functions.sh
index 56248d3e3ab..df83665cb66 100755
--- a/hadoop-common-project/hadoop-common/src/main/bin/hadoop-functions.sh
+++ b/hadoop-common-project/hadoop-common/src/main/bin/hadoop-functions.sh
@@ -1803,6 +1803,7 @@ function hadoop_start_daemon
     hadoop_error "ERROR:  Cannot write ${command} pid ${pidfile}."
   fi
 
+  CLASSPATH="${HADOOP_HOME}/../../../../Semantic-Daikon-Checker/target/*:$CLASSPATH"
   export CLASSPATH
   #shellcheck disable=SC2086
   exec "${JAVA}" "-Dproc_${command}" ${HADOOP_OPTS} "${class}" "$@"
diff --git a/hadoop-hdfs-project/hadoop-hdfs/pom.xml b/hadoop-hdfs-project/hadoop-hdfs/pom.xml
index bc6dabc5294..54683813400 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/pom.xml
+++ b/hadoop-hdfs-project/hadoop-hdfs/pom.xml
@@ -35,6 +35,13 @@ https://maven.apache.org/xsd/maven-4.0.0.xsd">
     <is.hadoop.component>true</is.hadoop.component>
   </properties>
 
+  <!-- <repositories>
+    <repository>
+        <id>local-maven-repo</id>
+        <url>file:///${project.basedir}/local-maven-repo</url>
+    </repository>
+  </repositories> -->
+
   <dependencies>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
@@ -207,6 +214,13 @@ https://maven.apache.org/xsd/maven-4.0.0.xsd">
         <artifactId>assertj-core</artifactId>
         <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>daikon</groupId>
+      <artifactId>Semantic-Daikon-Checker</artifactId>
+      <version>1.0</version>
+      <scope>system</scope>
+      <systemPath>/users/dimas/Semantic-Daikon-Checker/target/Semantic-Daikon-checker-1.0-SNAPSHOT-jar-with-dependencies.jar</systemPath>
+    </dependency>
   </dependencies>
 
   <build>
@@ -215,7 +229,11 @@ https://maven.apache.org/xsd/maven-4.0.0.xsd">
         <groupId>org.apache.maven.plugins</groupId>
         <artifactId>maven-surefire-plugin</artifactId>
         <configuration>
+          <argLine>-Xmx16G</argLine>
+          <parallel>suites</parallel>
+          <useUnlimitedThreads>true</useUnlimitedThreads>
           <systemPropertyVariables>
+            <daikon.app.target>hdfs</daikon.app.target>
             <startKdc>${startKdc}</startKdc>
             <kdc.resource.dir>${kdc.resource.dir}</kdc.resource.dir>
             <runningWithNative>${runningWithNative}</runningWithNative>
@@ -223,7 +241,7 @@ https://maven.apache.org/xsd/maven-4.0.0.xsd">
           <properties>
             <property>
               <name>listener</name>
-              <value>org.apache.hadoop.test.TimedOutTestsListener</value>
+              <value>t2c.TestListener</value>
             </property>
           </properties>
         </configuration>
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockIdManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockIdManager.java
index bec6ec83681..1e39cf82951 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockIdManager.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockIdManager.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;
 import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;
 import org.apache.hadoop.hdfs.server.namenode.FSEditLog;
+import daikon.GlobalState;
 
 import java.io.IOException;
 
@@ -98,6 +99,8 @@ public long upgradeLegacyGenerationStamp() {
       HdfsServerConstants.RESERVED_LEGACY_GENERATION_STAMPS);
 
     legacyGenerationStampLimit = generationStamp.getCurrentValue();
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
     return generationStamp.getCurrentValue();
   }
 
@@ -111,6 +114,8 @@ public void setLegacyGenerationStampLimit(long stamp) {
     Preconditions.checkState(legacyGenerationStampLimit ==
         HdfsConstants.GRANDFATHER_GENERATION_STAMP);
     legacyGenerationStampLimit = stamp;
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
   }
 
   /**
@@ -132,6 +137,8 @@ SequentialBlockIdGenerator getBlockIdGenerator() {
    */
   public void setLastAllocatedContiguousBlockId(long blockId) {
     blockIdGenerator.skipTo(blockId);
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
   }
 
   /**
@@ -148,6 +155,8 @@ public long getLastAllocatedContiguousBlockId() {
    */
   public void setLastAllocatedStripedBlockId(long blockId) {
     blockGroupIdGenerator.skipTo(blockId);
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
   }
 
   /**
@@ -163,6 +172,8 @@ public long getLastAllocatedStripedBlockId() {
    */
   public void setLegacyGenerationStamp(long stamp) {
     legacyGenerationStamp.setCurrentValue(stamp);
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
   }
 
   /**
@@ -177,9 +188,12 @@ public long getLegacyGenerationStamp() {
    */
   public void setGenerationStamp(long stamp) {
     generationStamp.setCurrentValue(stamp);
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
   }
 
   /**
+   * 
    * Set the currently highest gen stamp from active. Used
    * by Standby only.
    * @param stamp new genstamp
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
index 0510bd831f1..a44ba31ddee 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
@@ -162,6 +162,7 @@
  */
 @InterfaceAudience.Private
 public class BlockManager implements BlockStatsMXBean {
+  public static BlockManager blockManagerInstance = null;
 
   public static final Logger LOG = LoggerFactory.getLogger(BlockManager.class);
   public static final Logger blockLog = NameNode.blockStateChangeLog;
@@ -607,6 +608,8 @@ public BlockManager(final Namesystem namesystem, boolean haEnabled,
         DFSConfigKeys.DFS_NAMENODE_BLOCKREPORT_QUEUE_SIZE_DEFAULT);
     blockReportThread = new BlockReportProcessingThread(queueSize);
 
+    blockManagerInstance = this;
+
     LOG.info("defaultReplication         = {}", defaultReplication);
     LOG.info("maxReplication             = {}", maxReplication);
     LOG.info("minReplication             = {}", minReplication);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
index b1257705cd9..1b55389aed3 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
@@ -19,6 +19,7 @@
 
 import static org.apache.hadoop.util.Time.monotonicNow;
 
+import daikon.GlobalState;
 import java.io.Closeable;
 import java.io.EOFException;
 import java.io.IOException;
@@ -438,6 +439,8 @@ List<DatanodeCommand> blockReport(long fullBrLeaseId) throws IOException {
     }
     scheduler.updateLastBlockReportTime(monotonicNow());
     scheduler.scheduleNextBlockReport();
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
     return cmds.size() == 0 ? null : cmds;
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
index bc9fb13d8be..77f57628e9f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hdfs.server.datanode;
 
-
+import daikon.GlobalState;
 import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_ADDRESS_DEFAULT;
 import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_ADDRESS_KEY;
 import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY;
@@ -263,6 +263,7 @@
 public class DataNode extends ReconfigurableBase
     implements InterDatanodeProtocol, ClientDatanodeProtocol,
         TraceAdminProtocol, DataNodeMXBean, ReconfigurationProtocol {
+  public static DataNode instance = null;
   public static final Logger LOG = LoggerFactory.getLogger(DataNode.class);
   
   static{
@@ -435,6 +436,7 @@ private static Tracer createTracer(Configuration conf) {
     initOOBTimeout();
     storageLocationChecker = null;
     volumeChecker = new DatasetVolumeChecker(conf, new Timer());
+    instance = this;
   }
 
   /**
@@ -520,6 +522,7 @@ public Map<String, Long> load(String key) throws Exception {
 
     initOOBTimeout();
     this.storageLocationChecker = storageLocationChecker;
+    instance = this;
   }
 
   @Override  // ReconfigurableBase
@@ -736,6 +739,8 @@ ChangedVolumes parseChangedVolumes(String newVolumes) throws IOException {
       }
     }
 
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
     return results;
   }
 
@@ -1264,6 +1269,8 @@ public void reportBadBlocks(ExtendedBlock block) throws IOException{
    */
   public void reportBadBlocks(ExtendedBlock block, FsVolumeSpi volume)
       throws IOException {
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
     BPOfferService bpos = getBPOSForBlock(block);
     bpos.reportBadBlocks(
         block, volume.getStorageID(), volume.getStorageType());
@@ -1830,6 +1837,8 @@ public DatanodeRegistration getDNRegistrationForBP(String bpid)
     if(bpos==null || bpos.bpRegistration==null) {
       throw new IOException("cannot find BPOfferService for bpid="+bpid);
     }
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
     return bpos.bpRegistration;
   }
   
@@ -2307,6 +2316,8 @@ private void reportBadBlock(final BPOfferService bpos,
   void transferBlock(ExtendedBlock block, DatanodeInfo[] xferTargets,
       StorageType[] xferTargetStorageTypes, String[] xferTargetStorageIDs)
       throws IOException {
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
     BPOfferService bpos = getBPOSForBlock(block);
     DatanodeRegistration bpReg = getDNRegistrationForBP(block.getBlockPoolId());
 
@@ -3349,6 +3360,8 @@ public DatanodeID getDatanodeId() {
   @VisibleForTesting
   public void clearAllBlockSecretKeys() {
     blockPoolTokenSecretManager.clearAllKeysForTesting();
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
   }
 
   @Override // ClientDatanodeProtocol
@@ -3400,6 +3413,8 @@ public void checkDiskError() throws IOException {
     } else {
       LOG.debug("checkDiskError encountered no failures");
     }
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
   }
 
   private void handleVolumeFailures(Set<FsVolumeSpi> unhealthyVolumes) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
index fef71b51bad..a490f21982d 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
 
+import daikon.GlobalState;
 import java.io.BufferedOutputStream;
 import java.io.DataOutputStream;
 import java.io.EOFException;
@@ -1097,6 +1098,8 @@ void finalizeNewReplica(ReplicaInfo newReplicaInfo,
       newReplicaInfo.deleteMetadata();
       throw ioe;
     }
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
   }
 
   /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
index 259c6a61f17..ab7ba38cff1 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
@@ -327,6 +327,7 @@
 import com.google.common.collect.Lists;
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
 import org.slf4j.LoggerFactory;
+import daikon.GlobalState;
 
 /**
  * FSNamesystem is a container of both transient
@@ -407,6 +408,8 @@ private void logAuditEvent(boolean succeeded, String cmd, String src,
   private void logAuditEvent(boolean succeeded,
       UserGroupInformation ugi, InetAddress addr, String cmd, String src,
       String dst, FileStatus status) {
+    GlobalState.mode = GlobalState.T2CMode.TEST;
+    GlobalState.triggerEvent();
     final String ugiStr = ugi.toString();
     for (AuditLogger logger : auditLoggers) {
       if (logger instanceof HdfsAuditLogger) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
index 30bf4f85988..00bbe63592c 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
@@ -225,6 +225,8 @@ public class NameNode extends ReconfigurableBase implements
     HdfsConfiguration.init();
   }
 
+  public static NameNode instance = null;
+
   private InMemoryLevelDBAliasMapServer levelDBAliasMapServer;
 
   /**
@@ -990,6 +992,7 @@ protected NameNode(Configuration conf, NamenodeRole role)
       throw e;
     }
     this.started.set(true);
+    instance = this;
   }
 
   private void stopAtException(Exception e){
